{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7com28W55pHk"
      },
      "source": [
        "# Nutrition Image Analysis using CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whbZ5Uw35pHw"
      },
      "source": [
        "### Importing Neccessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "60eg6zmo5pHx"
      },
      "outputs": [],
      "source": [
        "import numpy as np#used for numerical analysis\n",
        "import tensorflow #open source used for both ML and DL for computation\n",
        "from tensorflow.keras.models import Sequential #it is a plain stack of layers\n",
        "from tensorflow.keras import layers #A layer consists of a tensor-in tensor-out computation function\n",
        "#Dense layer is the regular deeply connected neural network layer\n",
        "from tensorflow.keras.layers import Dense,Flatten\n",
        "#Faltten-used fot flattening the input or change the dimension\n",
        "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Dropout #Convolutional layer\n",
        "#MaxPooling2D-for downsampling the image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnVt93M05pH0"
      },
      "source": [
        "### Image Data Agumentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VLZKCTd5pH1"
      },
      "outputs": [],
      "source": [
        "#setting parameter for Image Data agumentation to the training data\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,shear_range=0.2,zoom_range=0.2,horizontal_flip=True)\n",
        "#Image Data agumentation to the testing data\n",
        "test_datagen=ImageDataGenerator(rescale=1./255)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpsHveuq5pH4"
      },
      "source": [
        "### Loading our data and performing data agumentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Hkc9ffd5pH5",
        "outputId": "f2f3a171-41c1-4f1c-e125-781a47760b45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 730 images belonging to 4 classes.\n",
            "Found 748 images belonging to 5 classes.\n"
          ]
        }
      ],
      "source": [
        "#performing data agumentation to train data\n",
        "x_train = train_datagen.flow_from_directory(\n",
        "    r'C:\\Users\\Harithan\\IBM_Proj\\Dataset\\TRAIN_SET',\n",
        "    target_size=(64, 64),batch_size=5,color_mode='rgb',class_mode='sparse')\n",
        "#performing data agumentation to test data\n",
        "x_test = test_datagen.flow_from_directory(\n",
        "    r'C:\\Users\\Harithan\\IBM_Proj\\Dataset\\TEST_SET',\n",
        "    target_size=(64, 64),batch_size=5,color_mode='rgb',class_mode='sparse') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szwYFmls5pH8",
        "outputId": "be341c75-9d14-446a-c340-8158106647ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'APPLES': 0, 'BANANA': 1, 'ORANGE': 2, 'WATERMELON': 3}\n"
          ]
        }
      ],
      "source": [
        "print(x_train.class_indices)#checking the number of classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SliKn605pH-",
        "outputId": "56cad51d-fcd8-4aca-d0b0-ed3b3c492b7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'APPLES': 0, 'BANANA': 1, 'ORANGE': 2, 'PINEAPPLE': 3, 'WATERMELON': 4}\n"
          ]
        }
      ],
      "source": [
        "print(x_test.class_indices)#checking the number of classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWWDoRDw5pIA",
        "outputId": "5e13f553-81ed-410f-b80d-35a378902983"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({0: 62, 1: 250, 2: 250, 3: 168})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from collections import Counter as c\n",
        "c(x_train .labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3R_JW4b5pIC"
      },
      "source": [
        "### Creating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eejmbWX75pID"
      },
      "outputs": [],
      "source": [
        "# Initializing the CNN\n",
        "classifier = Sequential()\n",
        "\n",
        "# First convolution layer and pooling\n",
        "classifier.add(Conv2D(32, (3, 3), input_shape=(64, 64, 3), activation='relu'))\n",
        "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Second convolution layer and pooling\n",
        "classifier.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "\n",
        "# input_shape is going to be the pooled feature maps from the previous convolution layer\n",
        "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Flattening the layers\n",
        "classifier.add(Flatten())\n",
        "\n",
        "# Adding a fully connected layer\n",
        "classifier.add(Dense(units=128, activation='relu'))\n",
        "classifier.add(Dense(units=5, activation='softmax')) # softmax for more than 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNCisXGE5pIE",
        "outputId": "3753599e-dc4d-4005-b574-50e3c2efc1fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 62, 62, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 31, 31, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 29, 29, 32)        9248      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 14, 14, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 6272)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               802944    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 5)                 645       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 813,733\n",
            "Trainable params: 813,733\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "classifier.summary()#summary of our model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTpQ5NR95pIF"
      },
      "source": [
        "### Compiling the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0sf79GD5pIH"
      },
      "outputs": [],
      "source": [
        "# Compiling the CNN\n",
        "# categorical_crossentropy for more than 2\n",
        "classifier.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6CAbE5c5pIL"
      },
      "source": [
        "## Fitting the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8JLV16x5pIM",
        "outputId": "69497dfa-dd17-4320-9c4d-b88e659ff75e",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Harithan\\AppData\\Local\\Temp\\ipykernel_2984\\549542485.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  classifier.fit_generator(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "146/146 [==============================] - 34s 205ms/step - loss: 0.8986 - accuracy: 0.6178 - val_loss: 0.9518 - val_accuracy: 0.6056\n",
            "Epoch 2/20\n",
            "146/146 [==============================] - 16s 108ms/step - loss: 0.5531 - accuracy: 0.7658 - val_loss: 0.5908 - val_accuracy: 0.7701\n",
            "Epoch 3/20\n",
            "146/146 [==============================] - 16s 109ms/step - loss: 0.4971 - accuracy: 0.7973 - val_loss: 0.7736 - val_accuracy: 0.6912\n",
            "Epoch 4/20\n",
            "146/146 [==============================] - 16s 110ms/step - loss: 0.4943 - accuracy: 0.8110 - val_loss: 0.9634 - val_accuracy: 0.6578\n",
            "Epoch 5/20\n",
            "146/146 [==============================] - 14s 98ms/step - loss: 0.4215 - accuracy: 0.8274 - val_loss: 0.8180 - val_accuracy: 0.7353\n",
            "Epoch 6/20\n",
            "146/146 [==============================] - 16s 111ms/step - loss: 0.3976 - accuracy: 0.8493 - val_loss: 0.7120 - val_accuracy: 0.7366\n",
            "Epoch 7/20\n",
            "146/146 [==============================] - 16s 108ms/step - loss: 0.3702 - accuracy: 0.8658 - val_loss: 1.0481 - val_accuracy: 0.6765\n",
            "Epoch 8/20\n",
            "146/146 [==============================] - 17s 114ms/step - loss: 0.3224 - accuracy: 0.8753 - val_loss: 0.6493 - val_accuracy: 0.7634\n",
            "Epoch 9/20\n",
            "146/146 [==============================] - 16s 111ms/step - loss: 0.3149 - accuracy: 0.8808 - val_loss: 0.6687 - val_accuracy: 0.7901\n",
            "Epoch 10/20\n",
            "146/146 [==============================] - 16s 108ms/step - loss: 0.2780 - accuracy: 0.9014 - val_loss: 0.5994 - val_accuracy: 0.7861\n",
            "Epoch 11/20\n",
            "146/146 [==============================] - 16s 110ms/step - loss: 0.2477 - accuracy: 0.9123 - val_loss: 0.5982 - val_accuracy: 0.7955\n",
            "Epoch 12/20\n",
            "146/146 [==============================] - 16s 110ms/step - loss: 0.2171 - accuracy: 0.9178 - val_loss: 0.7929 - val_accuracy: 0.7714\n",
            "Epoch 13/20\n",
            "146/146 [==============================] - 16s 108ms/step - loss: 0.2736 - accuracy: 0.8849 - val_loss: 0.6621 - val_accuracy: 0.8102\n",
            "Epoch 14/20\n",
            "146/146 [==============================] - 16s 109ms/step - loss: 0.2227 - accuracy: 0.9096 - val_loss: 0.7347 - val_accuracy: 0.7901\n",
            "Epoch 15/20\n",
            "146/146 [==============================] - 16s 110ms/step - loss: 0.2073 - accuracy: 0.9205 - val_loss: 0.7436 - val_accuracy: 0.7647\n",
            "Epoch 16/20\n",
            "146/146 [==============================] - 16s 109ms/step - loss: 0.1940 - accuracy: 0.9274 - val_loss: 0.6981 - val_accuracy: 0.7821\n",
            "Epoch 17/20\n",
            "146/146 [==============================] - 16s 112ms/step - loss: 0.2111 - accuracy: 0.9274 - val_loss: 0.9575 - val_accuracy: 0.7594\n",
            "Epoch 18/20\n",
            "146/146 [==============================] - 16s 108ms/step - loss: 0.1956 - accuracy: 0.9260 - val_loss: 0.8117 - val_accuracy: 0.7487\n",
            "Epoch 19/20\n",
            "146/146 [==============================] - 16s 108ms/step - loss: 0.1353 - accuracy: 0.9507 - val_loss: 0.9484 - val_accuracy: 0.7634\n",
            "Epoch 20/20\n",
            "146/146 [==============================] - 16s 110ms/step - loss: 0.1715 - accuracy: 0.9315 - val_loss: 1.0209 - val_accuracy: 0.7487\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x2a95a7a0b80>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier.fit_generator(\n",
        "        generator=x_train,steps_per_epoch = len(x_train),\n",
        "        epochs=20, validation_data=x_test,validation_steps = len(x_test))# No of images in test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icM7Nuc35pIO"
      },
      "source": [
        "### Saving our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAJYdsrl5pIQ"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "classifier.save('nutrition.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnKeLh5m5pIR"
      },
      "source": [
        "### Predicting our results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tJkyuyz5pIR"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import keras\n",
        "from tensorflow.keras.preprocessing import image\n",
        "model = load_model(\"nutrition.h5\") #loading the model for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSQ6tnsR5pIc",
        "outputId": "0ffb0001-eb70-42f8-d2e9-93cfde566c05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 346ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[False, False,  True, False, False]])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "img = keras.utils.load_img(r'C:\\Users\\Harithan\\IBM_Proj\\flask\\uploads\\Test_Image5.jpg',\n",
        "                     grayscale=False,target_size= (64,64))#loading of the image\n",
        "x = image.img_to_array(img)#image to array\n",
        "x = np.expand_dims(x,axis = 0)#changing the shape\n",
        "pred = (model.predict(x) > 0.5)#predicting the classes\n",
        "pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EazH0bQ05pIc",
        "outputId": "4ee9e9f9-f628-447c-80a9-9d4a86020316"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "only integer scalar arrays can be converted to a scalar index",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[1;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m index\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAPPLES\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBANANA\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mORANGE\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPINEAPPLE\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWATERMELON\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m result\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      3\u001b[0m result\n",
            "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
          ]
        }
      ],
      "source": [
        "index=['APPLES', 'BANANA', 'ORANGE','PINEAPPLE','WATERMELON']\n",
        "result=str(index[pred[0]])\n",
        "result"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}